---
title: "How to be Curious instead of Contrarian about COVID-19: Eight Data Science Lessons from 'Coronavirus Perspective' (Epstein 2020)"
output:
  html_notebook:
    toc: yes
date: 
author: 
affiliation: Director, Machine Learning for Social Science Lab, Center for Peace and Security Studies, University of California San Diego
---

<style type="text/css">
blockquote {
    padding: 10px 20px;
    margin: 0 0 20px;
    font-size: 14px;
    border-left: 5px solid #eee;
}
</style>

**Rex W. Douglass PhD** ^[If you found this note useful, please consider donating a few minutes to contribute examples of government COVID-19 quarentine measures to the [TIGR Project](https://github.com/rexdouglass/TIGR)]

[[@RexDouglass]](https://twitter.com/RexDouglass)

**3/24/2020** ^[Acknowledgments: I thank a good chunk of the NYU Law Class of 2013 for suggesting the subject of this review.]


[ROUGH DRAFT, PLEASE DO NOT CIRCULATE. COMMENTS WELCOME!]

```{r, echo=F, message=FALSE, results = FALSE, warning=FALSE}

library(tidyverse)
library(scales)
library(gghighlight)

```

```{r, echo=F, message=FALSE, results = FALSE, warning=FALSE}

library(lubridate)
library(tidyverse)
library(R0)  # consider moving all library commands to top -- this one was in a loop below
#Until the U.S. states one goes live have to pull it from here
confirmed_old <- read_csv(url("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/archived_data/archived_time_series/time_series_19-covid-Confirmed_archived_0325.csv")) #using the archived copy from today because they haven't posted the seperate U.S. data yet
deaths_old <- read_csv(url("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/archived_data/archived_time_series/time_series_19-covid-Deaths_archived_0325.csv"))
#recovered <- read_csv(url("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/archived_data/archived_time_series/time_series_19-covid-Recovered_archived_0325.csv"))
confirmed_old_long <- pivot_longer(confirmed_old, names_to = "date", cols = ends_with("20"), values_to = "confirmed")
deaths_old_long <- pivot_longer(deaths_old, names_to = "date", cols = ends_with("20"), values_to = "deaths")
#recovered_long <- pivot_longer(recovered, names_to = "date", cols = ends_with("20"), values_to = "recovered")

#
#https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv
confirmed <- read_csv(url("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv")) #using the archived copy from today because they haven't posted the seperate U.S. data yet
deaths <- read_csv(url("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv"))

confirmed_long <- pivot_longer(confirmed, names_to = "date", cols = ends_with("20"), values_to = "confirmed")
deaths_long <- pivot_longer(deaths, names_to = "date", cols = ends_with("20"), values_to = "deaths")
#recovered_long <- pivot_longer(recovered, names_to = "date", cols = ends_with("20"), values_to = "recovered")

WHO <- read_csv(url("https://raw.githubusercontent.com/datasets/covid-19/master/data/countries-aggregated.csv"))

#Push the Chinese count back to under 100
#https://en.wikipedia.org/wiki/2019%E2%80%9320_coronavirus_pandemic_in_mainland_China
early_chinese <- data.frame(
  date_asdate=ymd("2020-01-22")-7:1,
  confirmed=c(41,45,62,121,198,291,440),
  deaths=c(0,0,0,0,0,0,0)
)
early_chinese$country = "China"

all_long <- bind_rows(
              deaths_long %>% full_join(confirmed_long)  %>%
              #full_join(recovered_long) %>%
              #filter(confirmed>0) %>% 
            
              mutate(date_asdate = mdy(str_replace(date,"20$","2020"))) %>% 
              rename(country=`Country/Region`, state=`Province/State`) %>%
              filter(!country %in% c("From Diamond Princess","Diamond Princess","Cruise Ship")),
              
              early_chinese
            ) 



padded <- all_long %>%
            tidyr::expand( country, date_asdate=min(all_long$date_asdate)+0:223 ) %>% distinct()

countries_long <- padded %>% left_join(all_long) %>%
                  arrange(country, date_asdate) %>%
                  group_by(country,date_asdate) %>%
                    summarize(deaths=sum(deaths, na.rm=T), confirmed=sum(confirmed, na.rm=T)) %>%
                  ungroup() %>%
                  group_by(country) %>%
                    mutate(confirmed_cummax=cummax(confirmed)) %>%
                    mutate(days_since_1_confirmed=cumsum(confirmed_cummax>0)) %>%
                    mutate(days_since_10_confirmed=cumsum(confirmed_cummax>10)) %>%
                    mutate(days_since_50_confirmed=cumsum(confirmed_cummax>50)) %>%
                    mutate(days_since_100_confirmed=cumsum(confirmed_cummax>100)) %>%
                    mutate(days_since_500_confirmed=cumsum(confirmed_cummax>500)) %>%

                    mutate(deaths_cummax=cummax(deaths)) %>%        
                    mutate(days_since_1_deaths=cumsum(deaths_cummax>0)) %>%
                    mutate(days_since_10_deaths=cumsum(deaths_cummax>10)) %>%
                    mutate(days_since_50_deaths=cumsum(deaths_cummax>50)) %>%
                    mutate(days_since_100_deaths=cumsum(deaths_cummax>100)) %>%
                    mutate(days_since_500_deaths=cumsum(deaths_cummax>500)) %>%
        
                    mutate(confirmed_fd=confirmed-lag(confirmed)) %>%
                    mutate(deaths_fd=deaths-lag(deaths)) %>%
                  ungroup() %>%
                  arrange(country, date_asdate) %>%
      
                  #filter(days_since_1_confirmed>0) %>%
                  group_by(country) %>%
                    mutate(confirmed_max=max(confirmed)) %>%
                  ungroup() %>%
       
                  mutate(deaths=ifelse(deaths==0 & deaths_cummax>0, NA,deaths)) %>%
                  mutate(confirmed=ifelse(confirmed==0 & confirmed_cummax>0, NA,confirmed)) 

us_long <- deaths_old_long %>% full_join(confirmed_old_long)  %>%
                mutate(date_asdate = mdy(str_replace(date,"20$","2020"))) %>% 
                rename(country=`Country/Region`, state=`Province/State`) %>%
                filter(!country %in% c("From Diamond Princess","Diamond Princess","Cruise Ship")) %>% 
                filter(country %in% "US") %>%
                filter(state!="US")   #they went and added the US as a state
       
us_long <- padded %>%
           left_join(us_long) %>% 
          filter(country %in% "US") %>%
          filter(state!="US") %>%  #they went and added the US as a state
 
                  arrange(country, state,date_asdate) %>%
                  group_by(country,state,date_asdate) %>%
                    summarize(deaths=sum(deaths, na.rm=T), confirmed=sum(confirmed, na.rm=T)) %>%
                  ungroup() %>%
                  group_by(country,state) %>%
                    mutate(confirmed_cummax=cummax(confirmed)) %>%
                    mutate(days_since_1_confirmed=cumsum(confirmed_cummax>0)) %>%
                    mutate(days_since_10_confirmed=cumsum(confirmed_cummax>10)) %>%
                    mutate(days_since_50_confirmed=cumsum(confirmed_cummax>50)) %>%
                    mutate(days_since_100_confirmed=cumsum(confirmed_cummax>100)) %>%
                    mutate(days_since_500_confirmed=cumsum(confirmed_cummax>500)) %>%

                    mutate(deaths_cummax=cummax(deaths)) %>%        
                    mutate(days_since_1_deaths=cumsum(deaths_cummax>0)) %>%
                    mutate(days_since_10_deaths=cumsum(deaths_cummax>10)) %>%
                    mutate(days_since_50_deaths=cumsum(deaths_cummax>50)) %>%
                    mutate(days_since_100_deaths=cumsum(deaths_cummax>100)) %>%
                    mutate(days_since_500_deaths=cumsum(deaths_cummax>500)) %>%
        
                    mutate(confirmed_fd=confirmed-lag(confirmed)) %>%
                    mutate(deaths_fd=deaths-lag(deaths)) %>%
                  ungroup() %>%
                  arrange(country, date_asdate) %>%
      
                  #filter(days_since_1_confirmed>0) %>%
                  group_by(country, state) %>%
                    mutate(confirmed_max=max(confirmed)) %>%
                  ungroup() %>%
       
                  mutate(deaths=ifelse(deaths==0, NA,deaths)) %>%
                  mutate(confirmed=ifelse(confirmed==0, NA,confirmed)) 

us_states_long <- us_long %>% filter(!str_detect(state,","))
#table(us_states_long$country)
#table(us_states_long$state)

countries_and_us_states <- bind_rows(countries_long,
                                     us_states_long) %>%
                           mutate(prefered_label = ifelse( is.na(state) | state=='' | state=="US",   country, paste0(state, ", ", country) )) %>% 
                           arrange(prefered_label, date_asdate)

#table(countries_and_us_states$country)
#table(countries_and_us_states$state)
#table(countries_and_us_states$prefered_label)

#Were we double counting?

world_long <- deaths_long %>% 
              full_join(confirmed_long) %>% 
              full_join(recovered_long) %>%
              filter(confirmed>0) %>% 
            
              mutate(date_asdate = mdy(str_replace(date,"20$","2020"))) %>% 
              rename(country=`Country/Region`, state=`Province/State`) #%>%
              #filter(!state %in% c("From Diamond Princess","Diamond Princess")) %>%
  
world_long <- padded %>% left_join(all_long) %>%
                  group_by(date_asdate) %>%
                    summarize(deaths=sum(deaths, na.rm=T), confirmed=sum(confirmed, na.rm=T)) %>%
                  ungroup() %>%
                  arrange(date_asdate) %>%

                  mutate(confirmed_cummax=cummax(confirmed)) %>%
                  mutate(days_since_1_confirmed=cumsum(confirmed_cummax>0)) %>%
                  mutate(days_since_10_confirmed=cumsum(confirmed_cummax>10)) %>%
                  mutate(days_since_50_confirmed=cumsum(confirmed_cummax>50)) %>%
                  mutate(days_since_100_confirmed=cumsum(confirmed_cummax>100)) %>%
                  mutate(days_since_500_confirmed=cumsum(confirmed_cummax>500)) %>%

                  mutate(deaths_cummax=cummax(deaths)) %>%        
                  mutate(days_since_1_deaths=cumsum(deaths_cummax>0)) %>%
                  mutate(days_since_10_deaths=cumsum(deaths_cummax>10)) %>%
                  mutate(days_since_50_deaths=cumsum(deaths_cummax>50)) %>%
                  mutate(days_since_100_deaths=cumsum(deaths_cummax>100)) %>%
                  mutate(days_since_500_deaths=cumsum(deaths_cummax>500)) %>%
      
                  mutate(confirmed_fd=confirmed-lag(confirmed)) %>%
                  mutate(deaths_fd=deaths-lag(deaths)) %>%
                  arrange(date_asdate) %>%
  
                  mutate(deaths=ifelse(deaths==0, NA,deaths)) %>%
                  mutate(confirmed=ifelse(confirmed==0, NA,confirmed))


```

# Introduction 

How should non-epidemiologists publicly discuss COVID-19 data and models? When leaders and citizens are especially sensitive to signals on public health, what is our intellectual responsibility to produce or defer to the analysis of more expert speakers? I argue that our responsibility during crisis is the same as it was before, to do good work, to the best of our abilities, with the scientific principles of curiosity and honesty. Alternative shorthands like 'staying in your lane' are a poor decision rule for sorting good work from bad, and they ignore the very messy process that underlies real-world scientific inquiry. Lane-keeping is a poor way to learn and become a better consumer of expert findings, and gate-keeping is a missed opportunity to provide the public goods of feedback and review. To demonstrate the point, this note provides a detailed review of a recent piece "[Coronavirus Perspective](https://web.archive.org/web/20200319165522/https://www.hoover.org/research/coronavirus-isnt-pandemic)" (Epstein 2020a). By applying and illustrating data science principles point for point to this non-epidemiological take on epidemiological questions, it his hoped that the reader will take away not why they should avoid working on new topics but rather how they should approach those topics in an honest, curious, and rigurous way. 

# Epstein (2020)

Epstein ([2020a](https://web.archive.org/web/20200319165522/https://www.hoover.org/research/coronavirus-isnt-pandemic)) argues that the U.S. ought to shift from a loose shelter in place style quarantine to a more limited shelter in place for just vulnerable populations. He provides two primary rationales. First, the number of cases and number of deaths both in the U.S. and worldwide are likely to be small. Second, mortality for under 60 is relatively low. Together, the two ideas suggest that restrictions on all groups is overkill and perhaps some compromise weaker position is preferred. He reiterates this position in a number of [interviews](https://reason.com/video/dont-expect-millions-to-die-from-coronavirus-says-richard-epstein/). In a follow up piece Epstein ([2020b](https://web.archive.org/web/20200319165522/https://www.hoover.org/research/coronavirus-isnt-pandemic)) doubles down on the core argument that the direct health costs of the disease will be moderate and a weakened response should be preferred, "allowing the virus to run its course—is a better path forward for the economy." After the U.K. briefly flirted with this approach before rejecting it, the option is being circulated publicly at the federal level in the U.S. and this text specifically is reportedly [popular among some U.S. policy makers](https://www.washingtonpost.com/politics/trump-signals-growing-weariness-with-social-distancing-and-other-steps-advocated-by-health-officials/2020/03/23/0920ea0a-6cfc-11ea-a3ec-70d7479d83f0_story.html) and refered to as a [competing projection](https://thefederalist.com/2020/03/19/will-the-costs-of-a-great-depression-outweigh-the-risks-of-coronavirus/). For that reason it serves a prime case for consideration on how to think about non-epidemiologists talking about scientific question that are decidedly out of their lane.

# Lesson 1: Actually care about the answer to a question

Epstein ([2020a](https://web.archive.org/web/20200319165522/https://www.hoover.org/research/coronavirus-isnt-pandemic)) frames itself as being contrarian rather than curious about the true state of the world.

> Much of the current analysis does not explain how and why rates of infection and death will spike, so I think that it is important to offer a dissenting voice.

> These are deeply contrarian estimates.

> Perhaps my analysis is all wrong, even deeply flawed. But the stakes are too high to continue on the current course without reexamining the data and the erroneous models that are predicting doom.

Science is about being curious about the true state of the world, and through application of evidence and methods, forming new more true beliefs than we held the day before.  Contrarianism is not a search for truth, it's a search for political influence in a market that rewards diversity of opinion for diversity's sake. 

As a consumer of analysis, the second I can tell the author doesn't actually care about the answer to the underlying question, they're dead to me. Performative controversy, fake horse races, hypotheses that don't follow from theory, no examination of model fit or out of sample performance, and so on are immediate red flags the author doesn't actually care what the right answer is and so neither should you. 

As a producer of analysis, the struggle is how to think about and do science alongside actors who generate controversy for self interest using a lot of the same language as science. The only real solution is to learn how to tell good work from bad work no matter the wrapping. 

# Lesson 2: Pose a question and propose a research design that can answer it

We can frame Epstein's argument as a concrete research question: what will number of deaths from COVID-19 in the United States be by say September 1. To be concrete, here are our outcomes, confirmed COVID-19 confirmed cases and deaths compiled by [Johns Hopkins CSSE](https://github.com/CSSEGISandData/COVID-19).

```{r, echo=F, message=FALSE, results = F, warning=FALSE}

library(ggplot2)
p1 <- countries_long %>% filter(country %in% "US") %>% 
      filter(days_since_100_confirmed>0)  %>% ggplot() + geom_point(aes(x=date_asdate, y=confirmed), color="red")  + xlab("Date") + theme_bw() + scale_y_continuous(labels = comma_format()) +
      scale_x_date(labels = date_format("%m-%d"), breaks='months')

p2 <- countries_long %>% filter(country %in% "US") %>%
      filter(days_since_100_confirmed>0)  %>% ggplot() + geom_point(aes(x=date_asdate, y=deaths)) + xlab("Date") + theme_bw() + scale_y_continuous(labels =  comma_format()) +
      geom_vline(xintercept=ymd("2020-09-01") , linetype = "dashed", col="black") +
      annotate("text", x = ymd("2020-09-01")-4, y = 700, label = "Value at this date is what we want to know", color="black", size=2.5, angle=90) +
      scale_x_date(labels = date_format("%m-%d"), breaks='months')


```

```{r, echo=F, message=FALSE, results = T, warning=FALSE, fig.width=12, fig.height=6}
library(patchwork) ; #install.packages("patchwork")
patchwork <- (p1 + p2) 
patchwork + plot_annotation(
  title = 'United States Covid-19 Cases and Deaths'#,
  #subtitle = 'These 3 plots will reveal yet-untold secrets about our beloved data-set',
  #caption = ''
)
```
To make this easier to compare across time and across countries, let's log transform the outcome and change date to number of days since the 100th reported case. This puts our forcasting horizon at about 180 days from the start of the U.S. episode and still about a 160 days from now. 

```{r, echo=F, message=FALSE, results = F, warning=FALSE}

library(ggplot2)
library(cowplot)
p1 <- countries_long %>% filter(country %in% "US") %>% filter(days_since_100_confirmed>0) %>% ggplot() + geom_point(aes(x=days_since_100_confirmed, y=confirmed), color="red")  + xlab("Days Since 100 Confirmed") + theme_bw() + scale_y_log10(labels = comma)
p2 <- countries_long %>% filter(country %in% "US") %>% filter(days_since_100_confirmed>0) %>%  ggplot() + geom_point(aes(x=days_since_100_confirmed, y=deaths)) + xlab("Days Since 100 Confirmed") + theme_bw()  + scale_y_log10(labels = comma) +
      geom_vline(xintercept=180 , linetype = "dashed", col="black") +
      annotate("text", x = 180-5, y = 200, label = "Value at this date is what we want to know", color="black", size=2.5, angle=90)

```

```{r, echo=F, message=FALSE, results = T, warning=FALSE, fig.width=12, fig.height=6}
library(patchwork) ; #install.packages("patchwork")
patchwork <- (p1 + p2) 
patchwork + plot_annotation(
  title = 'United States Covid-19 Cases and Deaths',
  subtitle = 'Log scale, and days since 100 confirmed cases'#,
  #caption = ''
)

```

Two immediate things to take away are first, we are interested specifically in deaths and are forced to understand spread of all cases incidentally as a means to understand deaths. The second is that our forecasting horizon is **far**. A lot can happen between now and then, and experts have [wildly varying expectations](https://fivethirtyeight.com/features/experts-say-the-coronavirus-outlook-has-worsened-but-the-trajectory-is-still-unclear/) about what will actually happen in this window. Even though there is a great deal of expert certainty about the underlying mechanics, what will happen or more precisely what we will choose to let happen, are unknowns.

# Lesson 3: Use failures of your predictions to revise your model

In the first draft of the piece dated and posted March 16, 2020 Epstein ([2020a](https://web.archive.org/web/20200319165522/https://www.hoover.org/research/coronavirus-isnt-pandemic)) predicts the following about future counts of deaths:

> From this available data, it seems more probable than not that the total number of cases world-wide will peak out at well under 1 million, with the total number of deaths at under 50,000 (up about eightfold). In the United States, if the total death toll increases at about the same rate, the current 67 deaths should translate into about 500 deaths at the end. Of course, every life lost is a tragedy—and the potential loss of 50,000 lives world-wide would be appalling—but those deaths stemming from the coronavirus are not more tragic than others, so that the same social calculus applies here that should apply in other cases. 

This is great. It makes a sharp testable prediction that we can use to validate in a timely manner a radical alternate model of disease spread.^[Can you imagine if this turned out to be right? Someone has to be holding the winning lotto number, and picking long odds outcomes can be high risk high reward.]

When the fatality number passed 500, Epstein [edited](https://webcache.googleusercontent.com/search?q=cache:-qQ7VMpqFRUJ:https://www.hoover.org/research/coronavirus-isnt-pandemic+&cd=1&hl=en&ct=clnk&gl=us) the online copy of the original March 16th piece to read 5,000 instead and added a footnote

> From this available data, it seems more probable than not that the total number of cases world-wide will peak out at well under 1 million, with the total number of deaths at under 50,000 (up about eightfold). In the United States, if the total death toll increases at about the same rate, the current 67 deaths should reach about 5000 (or twn percent of my estimated world total, which may also turn out to be low). [See correction & addendum at the end of this essay.]^[Typos are verbatim.]

> Correction & Addendum, added March 24, 2020: That estimate is ten times greater than the 500 number I erroneously put in the initial draft of the essay, and it, too, could prove somewhat optimistic. But any possible error rate in this revised projection should be kept in perspective. The current U.S. death toll stands at 592 as of noon on March 24, 2020, out of about 47,000 cases. So my adjusted figure, however tweaked, remains both far lower, and I believe far more accurate, than the common claim that there could be a million dead in the U.S. from well over 150 million coronavirus cases before the epidemic runs its course.

And then published a [follow up note](https://reason.com/2020/03/24/richard-epstein-cops-to-a-stupid-gaffe-in-controversial-coronavirus-essay-that-caught-trump-admin-attention/) saying he really meant to type 2,500 the first time.

 > In my column last week, I predicted that the world would eventually see about 50,000 deaths from the novel coronavirus, and the United States about 500. These two numbers are clearly not in sync. If the first number holds, the total US deaths should be about 4 to 5 percent of that total, or about 2,000–2,500 deaths. The current numbers are getting larger, so it is possible both figures will move up in a rough proportion from even that revised estimate. 

This is not great.  This alters the prediction but does not bother to alter the logic which led to the calculation. Multiplying the then global death count by 8 would lead to a global prediction of 50,000 and so multiplying the then U.S. death count of 67 by 8 would be 536, hence the forecast of 500. Likewise the 50,000 global total is left unchanged. Simply adding zeros to the prediction every time it is proven wrong doesn't alter the underlying model given in the same sentence. 

Don't do this. When you feel comfortable enough to share your predictions publicly, create a concrete record and stick by it. You can always make new predictions based on new models, but don't go back and massage past predictions after the fact. The temptation to try to gaslight others (and yourself) that you were really right the entire time is too great.^[And hilarious.]

We can visually examine this prediction in light of the data up to now, first by extending the time period out to 100 days and second adding global counts 

```{r, echo=F, message=FALSE, results = F, warning=FALSE}


library(ggplot2)
library(cowplot)
p1 <- countries_long %>% filter(country %in% "US") %>% filter(days_since_100_confirmed>0)  %>% ggplot() +
          geom_point(aes(x=days_since_100_confirmed, y=confirmed), color="red") + 
          geom_point(aes(x=days_since_100_confirmed, y=deaths), color="black") + 
          xlab("Days Since 100 Confirmed") + 
          theme_bw()  + scale_y_log10(labels = comma)  + xlim(0,100) +
          geom_hline(yintercept = 500, stat = 'hline', linetype = "dashed", col="black") +
          geom_hline(yintercept = 5000, stat = 'hline', linetype = "dashed", col="black") +
          ggtitle("United States") + 
          annotate("text", x = 80, y = 700, label = "March 16 Prediction", color="black", size=2.5)  + 
          annotate("text", x = 80, y = 7000, label = "March 24 Prediction", color="black", size=2.5) + ylab("")

p2 <- world_long %>% filter(days_since_100_confirmed>0) %>% ggplot() +
          geom_point(aes(x=days_since_100_confirmed, y=confirmed), color="red") + 
          geom_point(aes(x=days_since_100_confirmed, y=deaths), color="black") + 
          xlab("Days Since 100 Confirmed") + theme_bw() + scale_y_log10(labels = comma) + xlim(0,100) +
          geom_hline(yintercept = 50000, stat = 'hline', linetype = "dashed", col="black") +
          ggtitle("World") + 
          annotate("text", x = 80, y = 65000, label = "March 16 Prediction", color="black", size=2.5)  + ylab("")


```

```{r, echo=F, message=FALSE, results = T, warning=FALSE, fig.width=12, fig.height=6}

library(patchwork) ; #install.packages("patchwork")
patchwork <- (p1 + p2) 
patchwork + plot_annotation(
  title = 'United States and Global Covid-19 Deaths',
  subtitle = "Epstein's Predicted Maximum Total Deaths (March 16 and then updated on March 24)",
  #caption = ''
)

```
These forecasts in light of the actual data trajectory should immediately give you pause. For the United States, it would require a very soon departure from the current exponential trend which isn't visible yet in either the confirmed or death trends. The U.S. trend in deaths is actually ticking up slightly here.

For the world, growth starts off exponential, levels off, and then goes exponential again as it reaches a new part of the world. For Epstein's 50k estimate to be true, the trend in Europe and the U.S. would have to start leveling off now, and there would have to not be an exponential growth when the disease fully hits Latin America, Africa, and South East Asia. As of this writing the world is already half way to that prediction of a million confirmed cases. It's not that these outcomes aren't possible, it's that it's unclear what in the time trend up until now or in the theory suggests that's what is about to happen.

```{r, eval=F,echo=F, message=FALSE, results = FALSE, warning=FALSE}

library(growthrates)
grow_logistic_yshift <- function(time, parms) {
  with(as.list(parms), {
    y <- (K * y0) / (y0 + (K - y0) * exp(-mumax * time)) + y_shift
    as.matrix(data.frame(time = time, y = y))
  })
}

#https://cran.r-project.org/web/packages/growthrates/vignettes/User_models.html
#time <- 1:10
#out <- grow_logistic_yshift(time, parms = list(y0 = 1, mumax = 0.5, K = 10, y_shift = 2))
#plot(time, out[, "y"], type = "b")

grow_logistic_yshift <- growthmodel(grow_logistic_yshift, c("y0", "mumax", "K", "y_shift"))
fit <- fit_growthmodel(grow_logistic_yshift,
                       p = c(y0 = 1, mumax = 0.1,  K=5000, y_shift = 1),
                       time = us_long %>% filter(days_since_1_confirmed>1 & !is.na(deaths)) %>% pull(days_since_1_confirmed),
                       y = us_long %>% filter(days_since_1_confirmed>1 & !is.na(deaths)) %>% pull(deaths) )
plot(fit)
us_long$y_hat_deaths <- predict(fit,time=us_long$days_since_1_confirmed)[,'y']

fit <- fit_growthmodel(grow_logistic_yshift,
                       p = c(y0 = 1, mumax = 0.1, K=5000, y_shift = 1),
                       which=c("y0", "mumax", "y_shift"),
                       time = us_long$days_since_1_confirmed, y = us_long$deaths)
plot(fit)


```

# Lesson 4: Form meaningful prior beliefs with a thorough literature review

Thanks to the growing revolution in Open Science and preprint outlets like medrxiv.org and arxiv.org, the barrier to performing a decent review of recent COVID-19 research is membership in an academic institution with Google Scholar access, like Starbucks, South West Airlines, or your bathroom.

**Don't use straw men to represent the state of the art**

The piece begins with an incorrect and on face unlikely summary of the current state of the art

> Right now, the overwhelming consensus, based upon the most recent reports, is that the rate of infection will continue to increase so that the most severe interventions are needed to control what will under the worst of circumstances turn into a high rate of death. 

> Much of the current analysis does not explain how and why rates of infection and death will spike, so I think that it is important to offer a dissenting voice.

Only two citations are provided. The [first](https://web.archive.org/web/20200319181206/https://www.nytimes.com/interactive/2020/03/13/opinion/coronavirus-trump-response.html) is an educational infographic in the opinion section of the NYT. It includes a direct quote from the scientific consultants not to interpret the model as a production forecast

> “The point of a model like this is not to try to predict the future but to help people understand why we may need to change our behaviors or restrict our movements, and also to give people a sense of the sort of effect these changes can have,”

The [second](https://web.archive.org/web/20200319202026/https://medium.com/@tomaspueyo/coronavirus-act-today-or-people-will-die-f4d3d9cd99ca) is a medium blog post, by an MBA/engineer who runs an education website, and collects a number of plots and infographics on COVID-19 from around the web.

If these two examples are representative of something other than the scientific consensus, e.g. media commentary, then they should be cited specifically in that context and then followed by the long list of actual state of the art academic research that ought to be in the popular discourse. Instead, these two sources are presented as straw men to justify opining an alternative view.

**Don't claim the state of the art ignores factors that it actually takes into account**

Epstein's central criticism is that the epidimiological models at the heart of the scientific consensus share an absurd and incorrect assumption that R0 remains constant over time. He thinks that the R0 at the start of the crisis, is held constant for the entire time, and that's why estimated infection and casualty rates are so high.

Plainly, no. That's completely and obviously wrong. The models at the heart of the scientific consensus around COVID-19 take into account a decline in R0 over time.

The easiest way to tell is to simply think through what a time constant R0 implies, the disease's progression never stops, the entire population of 330 million Americans catches it, with a fatality rate of 1% producing 3.3 million fatalities.

The NYT's infographic he cites gives a infected rate of 100 million, and 1 million dead. So R0 mechanically must not remain constant, it must decline from it's initial values (initial R0=2.3 in that model).

The second easiest way to tell that's completely wrong is to actually read the article where it directly specifies a schedule of R0 reduction based on "mild intervention"

> The mild intervention as modeled here is where we are now in the United States: It is a status quo in which some gatherings are canceled and there is promotion of social distancing and work from home, but with inadequate testing and unaddressed supply shortages.

The third easiest way to tell that's completely wrong, is to look at literally any of the models available online or in published research and see how they handle R0 over time.

So for example [COVID-19 Scenarios](https://neherlab.org/covid19/) out of the University of Basel allows you to choose between strong, moderate, weak, none, or a user customizable schedule rate of R0 decline. Epidemic Envisioner in development at MIT has 7 different possible decay functions for R0 to choose from. The [Epidemic Calculator](http://gabgoh.github.io/COVID/index.html) allows you to vary the timing of an intervention and its effect on R0. The [Swiss-epidemic-model](https://ispmbern.github.io/covid-19/swiss-epidemic-model/) explicitly compares the effect of reducing transmission rates starting today over a range of values from 0 to 100%.

The agent based model behind the [Imperial College study](https://www.imperial.ac.uk/media/imperial-college/medicine/sph/ide/gida-fellowships/Imperial-College-COVID19-NPI-modelling-16-03-2020.pdf) examines many different adaptive non-government strategies including case isolation in the home after individual symptoms, voluntary home quarantine if anyone has symptoms, social distancing for those over 70, and social distancing for the entire population, still producing 81% infection rates and 2.2 million deaths. The [Framework for Reconstructing Epidemiological Dynamics (FRED)](https://www.post-gazette.com/news/health/2020/03/27/COVID-19-coronavirus-University-Pittsburgh-hospitalization-FRED-pandemic-model-case-study/stories/202003230067) out of the University of Pittsburgh is also an agent based that takes into acount adaptive response that drive R0 down over time. Or this [stochastic transmission model](https://www.thelancet.com/journals/laninf/article/PIIS1473-3099(20)30144-4/fulltext) fit to data from Wuhan and has a time varying R0. Prem et al. [2020](https://www.thelancet.com/journals/lanpub/article/PIIS2468-2667(20)30073-6/fulltext) simulate lifting the control measures in Wuhan, with a fixed R0, but transmission modeled directly by intergenerational mixing as restrictions are lifted again over time. 

In general, COVID-19 models, certainly models in production, take into account changes in R0 over time. More precisely, the high end models simulate the direct behaviors that lead to transmissions, of which R0 is a summary statistic. Default parameters are usually set to current levels of mitigation. The completely unmitigated case is no longer a relevant counterfactual, as the world now knows about the disease, and the relevant policy choices are between greater and less degrees of mitigation.

**Don't Get basic facts wrong**

The piece repeats twice the incorrect fact that COVID-19 has a "relatively short (two-week) incubation period." [Even](https://www.sciencedirect.com/science/article/pii/S1201971220301193) , [a](https://www.medrxiv.org/content/10.1101/2020.02.21.20026559v1) , [cursory](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3539694) , [search](https://www.medrxiv.org/content/10.1101/2020.02.19.20025452v4) would point to estimates of an incubation period of about 5 days, and an even shorter serial interval of about 4.5 days.

Better yet, when starting research on a new area, reading one of the many existing [literature reviews](https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)30567-5/fulltext) for lay audiences would explain the difference between a serial interval and an incubation period, the time until infectious and the time until symptomatic, and that COVID-19 is particularly dangerous precisely because many people can pass on the disease prior to knowing they have it.

**Don't cherry pick data**

Epstein (2020) chooses to report the Case Fatality Rate of South Korea because of its relatively low estimate of 0.92% but ostensibly because its more comprehensive testing makes it more accurate.

> It is instructive to see how this analysis fares by taking into account the Korean data, which is more complete than the American data. South Korea has been dealing with the coronavirus since January 20. Since that time, the Korean government has administered a total of 261,335 tests to its citizens. In press releases updated every day, the Korean CDC is reporting (as of March 15) 8,162 total infections against 75 deaths for an overall mortality rate of 0.92 percent. 

Selecting on data quality is problematic because South Korea's draconian testing regime is part of its success in combating COVID-19 and its low CFR. Looking for your car keys under the streetlight not because that's where they are but that's where you can see is a universally problematic approach to scientific inquirry, especially so where measurement, governance, and disaster all are strongly related. There are [heroic attempts](https://www.medrxiv.org/content/medrxiv/early/2020/03/06/2020.03.04.20031104.full.pdf) to combine several different kinds of data taking into account [undercounting](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4707560/) in other relevant cases like China which are finding CFR's still north of 1% at 1.6%. 

Cherry picking a case with good data and good news also misrepresents certainty over the measure. Estimating the Case Fatality Ratio CFR is difficult and can change in [either direction](https://www.rand.org/blog/2020/03/estimates-of-covid-19s-fatality-rate-might-change-and.html) over time as data come in. The CFR may not be knowable [for months](https://www.hindawi.com/journals/cmmm/2012/978901/).  

Because of this actual CFR estimates for COVID-19 [vary wildly across countries](https://www.cebm.net/global-covid-19-case-fatality-rates/). 


**Establish Underlying Mechanisms**

There are at least two types of fatalities. The first are persons who receive adequate medical care but still die anyway. The second are persons who would have lived but received insufficient or degraded medical care.

There is a vein of research on why COVID-19 actually kills you, which in addition to age finds specific vulnerabilities like hypertension, heart disease, diabetes, [cerebrovascular disease](https://www.medrxiv.org/content/10.1101/2020.02.24.20027268v1), cancer, chronic respiratory disease, and [kidney impairment](https://www.medrxiv.org/content/10.1101/2020.02.18.20023242v1). 

This is in turn because, the mechanism by which COVID-19 kills includes pneumonia as well as [myocarial](https://www.medrxiv.org/content/10.1101/2020.02.26.20028589v1) [injury](https://www.nature.com/articles/s41569-020-0360-5). It may also [damage the liver](https://www.thelancet.com/journals/langas/article/PIIS2468-1253(20)30057-1/fulltext).

Damage to the lungs necessitates oxygen, ventilation, and even intabation which places an enormous strain on healthcare resources. 

Hospitilization rates were a quarter of known cases in early U.S. CDC data of [25%](https://www.independent.co.uk/news/world/americas/coronavirus-new-york-hospital-cases-today-map-decrease-covid-19-a9425111.html), and are currently [12% in New York](https://www.independent.co.uk/news/world/americas/coronavirus-new-york-hospital-cases-today-map-decrease-covid-19-a9425111.html)

A proper literature review would find [growing](https://www.thelancet.com/journals/langlo/article/PIIS2214-109X(20)30068-1/fulltext#fig1) , [evidence](https://www.medrxiv.org/content/10.1101/2020.02.26.20028472v3) for a relationship between available health resources and mortality. Including that Italy took the start of the disease [quite seriously](https://jamanetwork.com/journals/jama/fullarticle/2763188).

[Early U.S. data](https://www.cdc.gov/mmwr/volumes/69/wr/mm6912e2.htm) show older than 65 accounted for 80% of deaths but only accounted for 45% of hospitalizations and 53% of ICU admissions. 

All together, this would point out that the proposed policy of sheltering in place only older Americans  wouldn't even remove half of the current burden on the healthcare system, much less when most adults under 60 are contracting the disease together 10% of them are all headed to the hospital at roughly the same time.

# Lesson 5: Don't form strong prior beliefs based on cherry picked data

In setting up our research design, we want to predict U.S. cases but we want to base that prediction on a model fit more broadly on other representative data. 

For example, Epstein sets up this small comparison between early U.S. death rates in Washington to peak death rates in Italy or China.

> What, then, does all of this portend for the future of COVID-19 in the United States? Good news is more likely than bad, notwithstanding the models that predict otherwise. The deaths in Washington have risen only slowly, even as the number of infections mount. 

We should instead phrase this as a question, how does the U.S.'s growth rate in COVID-19 deaths compare to elsewhere? Is early data from one or more states likely to be representative of the U.S. average rate? To answer this question, we compare each country and each U.S. state side by side, aligning their episodes by the first date each crossed 100 confirmed cases. We fit a linear trend line to the cumulative count of deaths in each country (log transformed). From the slope of that line we can calculate the day over day average percent change in deaths in each location.

```{r, echo=F, message=FALSE, results =F, warning=FALSE}

require(scales)
library(gghighlight); #install.packages('gghighlight')
options(gghighlight_max_labels=1000)

countries_and_us_states_first30 <- countries_and_us_states %>% filter(days_since_100_confirmed>0) %>% filter(days_since_100_confirmed<30)  %>% filter(!is.na(deaths)) #%>%
                     #dplyr::filter(prefered_label %in% c("Washington, US","US","Italy","China")) #if the last dot is NA it won't label it!

us_and_us_states_first30 <- countries_and_us_states_first30 %>% filter(country=="US")
  
library(dplyr)
library(broom)
dfHour = countries_and_us_states_first30 %>% dplyr::select(prefered_label,confirmed, deaths, confirmed_max, days_since_100_confirmed ) %>% na.omit() %>% group_by(prefered_label) %>% do(fitHour = lm(log(deaths+1) ~ days_since_100_confirmed, data = .))
dfHourCoef = tidy(dfHour, fitHour) %>% dplyr::filter(term %in% "days_since_100_confirmed") %>% ungroup() 
dfHourCoef$percent_change <- round((exp(dfHourCoef$estimate)-1)*100,2) 
slopes_countries_and_us_states_first30 <- dfHourCoef %>% dplyr::select(prefered_label, percent_change) %>% 
                                           mutate(prefered_label=fct_reorder(prefered_label, percent_change) ) 
                                          #%>% filter(!percent_change == 0)

slopes_countries_and_us_states_first30 %>% filter(!str_detect(prefered_label,"US$")) %>%
  filter(percent_change!=0) %>% pull(percent_change) %>% quantile()

countries_and_us_states_first30_covariates <- countries_and_us_states_first30 %>%
                                              left_join(slopes_countries_and_us_states_first30) %>% 
                                              mutate(prefered_label=fct_reorder(prefered_label, percent_change) ) 

p_slopes_countries_and_us_states_first30 = slopes_countries_and_us_states_first30  %>%
                                           filter(!percent_change == 0)  %>% 
                                           ggplot(aes(x=percent_change, y=prefered_label, color=prefered_label)) + geom_point() + 
                                           xlab("Average Day over Day Percent Growth in Deaths (First 30 Days)") + theme_bw() + ylab("")  + 
  
                                           gghighlight(prefered_label %in% c("Washington, US","Italy","China","US") , #"New York, US" #, "US"
                                                        label_params =
                                                          list(
                                                         size = 3,
                                                         segment.alpha=0)
                                                      )

#p_slopes_countries_and_us_states_first30 = slopes_countries_and_us_states_first30 %>%
#                                           ggplot() + geom_density(aes(x=percent_change)) + 
#                                           xlab("Average Day over Day Percent Growth in Deaths (First 30 Days)") + theme_bw() 

p1 <- countries_and_us_states_first30_covariates %>%
       ggplot() + 
       guides(color=FALSE) +
       geom_line(data=countries_and_us_states_first30_covariates, 
                aes(x=days_since_100_confirmed, y=deaths, color=prefered_label )) +
       #geom_label(data=countries_and_us_states_first30, 
       #        aes(x=days_since_100_confirmed, y=deaths, color=prefered_label )) %>%
       geom_smooth(data=countries_and_us_states_first30_covariates %>%
                     dplyr::filter(prefered_label %in% c("Washington, US","US","Italy","China")), #"New York, US"
                     aes(x=days_since_100_confirmed, y=deaths, color=prefered_label) , method = lm, se = FALSE, linetype = "dashed") + 
  
       gghighlight(prefered_label %in% c("Washington, US","Italy","China","US") , #"New York, US" #, "US"
                    label_params =
                      list(
                     size = 3,
                     segment.alpha=0)
                  ) +
       scale_x_continuous(breaks=seq(0, 30, by = 5)) + 
       scale_y_log10(labels = comma_format()) + 
  
       xlab("Days Since 100 Confirmed") + 
       ylab("Number of Deaths") +
       theme_bw()


p2 <- us_and_us_states_first30 %>% 
       ggplot() + 
       guides(color=FALSE) +
       geom_line(data=us_and_us_states_first30, 
                aes(x=days_since_100_confirmed, y=deaths, color=prefered_label )) +
       #geom_label(data=countries_and_us_states_first30, 
       #        aes(x=days_since_100_confirmed, y=deaths, color=prefered_label )) %>%
       geom_smooth(data=us_and_us_states_first30 %>%
                     dplyr::filter(prefered_label %in% c("Washington, US","US","New York, US")), #
                     aes(x=days_since_100_confirmed, y=deaths, color=prefered_label) , method = lm, se = FALSE, linetype = "dashed") + 
  
       gghighlight(#prefered_label %in% c("Washington, US","Italy","China") , #"New York, US" #, "US"
                    label_params =
                      list(
                     size = 3,
                     segment.alpha=0)
                  ) +
       scale_x_continuous(breaks=seq(0, 30, by = 5)) + 
       scale_y_log10(labels = comma_format()) + 
  
       xlab("Days Since 100 Confirmed") + 
       ylab("Number of Deaths") +
       theme_bw()




```


```{r, echo=F, message=FALSE, results =T, warning=FALSE, fig.width=12, fig.height=10}

library(patchwork) ; #install.packages("patchwork")
patchwork <- p1 +  p_slopes_countries_and_us_states_first30
patchwork + plot_annotation(
  title = 'National Growth Rate in COVID-19 Deaths',
  subtitle = ""#,
  #caption = ''
)

```

The plot on the left shows the raw data for countries with more than 100 cases, with coloring and trend lines fit to the 4 suggested case comparisons, Italy, China, Washington State, and the U.S. as a whole. The plot on the right shows the calculated average percentage daily change in deaths for the same.

On the first question, U.S. growth rate in deaths is 21.6% which is nearly exactly the median across countries 20.8%. The U.S. rate is below either Italy (29.32%) or China (29.58%), but just below.

On the second point, Washington State was and continues to be an example of lower growth in death rates (11.7%), which at half of the average U.S. rate is not representative of the country as a whole.^[Some of these states entered their episode after the article was posted on March 16th. Where possible I will try to give leeway in that regard, but on the other hand one of the immediate consequences of cherry picking early data from one state is that you'll be shown immediately wrong when a few more days of data come in.]

The piece goes on to say

> The New York cases have been identified for long enough that they should have produced more deaths if the coronavirus was as dangerous as is commonly believed.

I don't entirely know what this sentence is supposed to mean, but New York's day on day growth in fatalities is the third highest in the world in these data, just behind Massachusetts and Turkey, at 52.28%.

The lesson here being that intentionally looking for early "good news" means necessarily ignoring actual news relevant to the research question.^[ Left as an exercise to the reader to see how hard exactly you have to squint before you see "good news" from these data.]


```{r, eval=F , echo=F, message=FALSE, results =T, warning=FALSE}

p2 <- slopes %>% ggplot(aes(x=estimate)) + geom_density() +
  geom_vline(xintercept=slopes %>% filter(country=="US") %>% pull(estimate) ) + 
  annotate("text", x = slopes %>% filter(country=="US") %>% pull(estimate), y = 2.5, label = "U.S.", color="black", size=2.5) +
  geom_vline(xintercept=slopes %>% filter(country=="China") %>% pull(estimate) ) + 
  annotate("text", x = slopes %>% filter(country=="China") %>% pull(estimate), y = 2.5, label = "China", color="black", size=2.5) +
  geom_vline(xintercept=slopes %>% filter(country=="Italy") %>% pull(estimate) ) + 
  annotate("text", x = slopes %>% filter(country=="Italy") %>% pull(estimate), y = 2.5, label = "Italy", color="black", size=2.5) +
  xlab("Growth Rates in Deaths over first 30 days") + theme_bw() 

```

```{r, eval=F, echo=F, message=FALSE, results = T, warning=FALSE}

library(patchwork) ; #install.packages("patchwork")
patchwork <- (p2 ) 
patchwork + plot_annotation(
  title = 'Italy Covid-19 Confirmed and Deaths',
  subtitle = "",
  #caption = ''
)

```

# Lesson 6: Be specific and concrete about your theory 

To proceed further, we need to attempt to distill Richard Epstein's Model of Epidemiology and Disease (here after REMED). Getting REMED requires holding several simple but contradictory ideas at the same time.

**Implied Variables**

The first idea is that the one full cycle of COVID-19 spread and decline observed so far, China, is representative of future inflection points for other countries. Overlay the China curve, the China curve breaks, so another country's curve should break similarly too. (*CHINA_INFLECTION*)

> Overlooked is the good news coming out of China, where the latest report shows 16 new cases and 14 new deaths, suggesting that the number of deaths in the currently unresolved group will be lower than the 5.3 percent conversion rate in the cases resolved to date. In my view, we will see a similar decline in Italy, for reasons that I shall outline in the remainder of this article.

> In dealing with this point, it is critical to note that the rapid decline in the incidence of new cases and death in China suggests that cases in Italy will not continue to rise exponentially over the next several weeks.

The second is that, in contrast, the high death rate of China is not representative, it's idiosyncratic and not what should be expected in other countries because of its high rate of smoking and pollution (*SMOKERS*, *POLLUTION*).

> My own guess is that the percentage of deaths will decline in Korea for the same reasons that they are expected to decline in the United States. It is highly unlikely that there will ever be a repetition of the explosive situation in Wuhan, where air quality is poorer and smoking rates are higher.

Italy's death rate is also not representative, it's idiosyncratic and not what should be expected in other countries. (*Italy?*)^[The implied variable is unknown because neither the sentence nor linked NYT article provides an argument for why the U.S. health system is different or should perform better than the Italian health system. The implied theoretical variable remains a mystery. https://web.archive.org/web/20200319191049/https://www.nytimes.com/2020/03/12/world/europe/12italy-coronavirus-health-care.html]

> Moreover, it is unlikely that the healthcare system in the United States will be compromised in the same fashion as the Italian healthcare system in the wake of its quick viral spread.

Third, China's heavy handed policy response is not why its growth curve in deaths broke, and presumably neither will Italy's heavy handed policy response be responsible when it breaks as well. The U.S. should not copy them. (*POLICY*)

> As of March 16, the data from the United States falls short of justifying the draconian measures that are now being implemented. As of two days ago, 39 states have declared states of emergency, and they have been joined at the federal level with President Trump’s recent declaration to the same effect. These declarations are meant to endow governments with the power to impose quarantines and travel bans, close schools, restrict public gatherings, shut down major sporting events, stop public meetings, and close restaurants and bars. Private institutions are imposing similar restrictions. The one-two punch of public and private restrictions has caused a huge jolt to the economy.

> The irony here is that even though self-help measures like avoiding crowded spaces make abundant sense, the massive public controls do not. In light of the available raw data, public officials have gone overboard. 

But policy works

> Various institutional measures, both private and public, have also slowed down the transmission rate.

> The amount of voluntary and forced separation in the United States has gotten very extensive very quickly, which should influence rates of infection sooner rather than later.

Forth, the growth and decline curves of COVID-19 are a function of time, and naturally will burn itself out. Because of 

Societal Response (*ADAPTATION*)

> But once people are aware of the disease, they will start to make powerful adaptive responses, including washing their hands and keeping their distance from people known or likely to be carrying the infection. Various institutional measures, both private and public, have also slowed down the transmission rate.

Seasonality (*SEASONALITY*)

> And finally, the model explicitly ignores the possibility that the totals will decline as the weather gets warmer.

Natural selection will breed weaker COVID-19 (*WEAKENING*)

>At some tipping point, the most virulent viruses will be more likely to kill their hosts before the virus can spread. In contrast, the milder versions of the virus will wreak less damage to their host and thus will survive over the longer time span needed to spread from one person to another. Hence the rate of transmission will trend downward, as will the severity of the virus. It is a form of natural selection.

> Given that the coronavirus can spread through droplets and contact, the consequences of selection should manifest themselves more quickly than they did for AIDS.

Natural selection will remove weaker humans (*SUSCEPTIBLE*)

> Nor does the model recognize that if the most vulnerable people are hit first, subsequent iterations will be slower because the remaining pool of individuals is more resistant to infection.

**Summary**

To get REMED means to believe that the one case, China, predicts future trends in COVID-19 cases in the U.S. and elsewhere, but neither China nor Italy predict future levels of deaths in the U.S. and elsewhere, and the mechanism behind the trend in China was more nature than policy and so the U.S. can avoid enacting those policies and still expect to see the same drop in cases.

Let that sink in.

# Lesson 7: Choose enough cases to actually test your theory

We can formalize REMED as a series of equations

The rate of deaths at time t is a function of the distance before or after when China's inflection took place, the degree of societal adaptation, how much the disease has evolved to become less deadly, the number of suceptible people still left in the population, and the season.

$\partial DEATHS_t =F(CHINAINFLECTION, ADAPTATION,WEAKENING,SUSCEPTIBLE,SEASONALITY, Time)$

The total number of expected DEATHS however is just a function policy chosen, smoking population, pollution, and whether or not you're Italy.

$DEATHS=F(Policy, Smokers, Polution, Italy)$

Almost all social sciences grad students will be forced to read [King, Keohane, and Verba (1997)](https://press.princeton.edu/books/paperback/9780691034713/designing-social-inquiry) or something similar in their first year which details how to do small n case selection with an eye toward having enough cases and the right cases to test your explanations. We can express this directly by coding the available cases and putting them into a table.

```{r, eval=F, echo=F, message=FALSE, results = F, warning=FALSE}

#install.packages('datapasta')
library(datapasta)
x <- data.frame(
  country=c('China','Italy','U.S.'),
  deaths=c('High','High','?'),
  policy=c('Shelter in Place','Shelter in Place','Shelter in Place'),
  pollution=c('High','Low','Low'),
  smoking=c('2043','1493.3','1016.6'),
  Italy=c('No','Yes','No')
)

library(knitr)
library(kableExtra); #install.packages('kableExtra')

```

```{r, eval=T, echo=F, message=F, results = T, warning=FALSE}
x %>%
  kable() %>%
  kable_styling()

```

A number of problems should become immediately apparent. First, the number of observations with known deaths is just 2, China and Italy. But the number of explanations we hope to test with them is 4. We don't have enough degrees of freedom to mathematically demonstrate a relationship between total deaths and every one of these explanations.

Second, we don't have any variation on the dependent variable, both Italy and China have a high death tolls.^[What counts as low is a moving target because the original piece predicted only 500 for the U.S. and said it wasn't likely to reach the high numebr of China's because China had more smokers. Now Italy's high death count is the new high. But he forecasts 5,000 for the U.S. which would then make it the new high. But this is contrast to the NYT estimate of a million, so maybe these are all really low. In either case, there's still no variation on the dependent variable. ] The sample does not include any examples of low death toll countries for us to infer from.

Third, we don't have variation on the independent variable of interest, policy, either. All three of these countries have shelter in place style policies of one form or another. What variation there is, both China's and Italy's are more extreme the U.S. so far.

A proposal for how to salvage this design is left here as an exercise for the reader. First, collapse smoking and pollution into a single lung health index. Second, replace the Italy dummy with an actual theoretical explanation for their healthcare system struggled but the U.S. for some reason won't. Third, expand the sample of cases to include variation on both the dependent AND independent variable. That is, we need examples of countries with low death totals. For example, Singapore, Hong Kong, and Taiwan. We also need examples of countries with weak policies, no shelter in place. After assembling a moderate number of examples, we could then see whether at least the correlation between policy and death totals is zero.

Looking at those cases, we might desire to consider other measurements suggested by the literature review regarding the composition of the healthcare system, strategies other than shelter in place like aggressive testing and tracing, authoritarian proclamations that there is no disease and no deaths have been because of it, etc.

This would still not establish the causal relationships desired, but it would at least but mathematically possible to make the implied comparisons. As it stands now, the inferences are nonsensical.

# Lesson 8: Convey uncertainty with specificity not doublespeak

The piece presents itself as if its communicating confidence and uncertainty, e.g. using the words likely/unlikely/probable 8 times. There are mathematical ways of explicitly communicating uncertainty, and where that uncertainty lies in measurement, parameter, or prediction. There are even qualitative ways of communicating uncertainty to a qualitative audience, e.g. the CIA's [desperate mapping](https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/books-and-monographs/sherman-kent-and-the-board-of-national-estimates-collected-essays/6words.html) from probabilities to adjectives for lay policymakers. 

There is also a wrong way to convey uncertainty which is to pepper your language with contradictory hedging and doublespeak that generates uncertainty in the reader's mind about what you actually mean. 

For example:

> That estimate is ten times greater than the 500 number I erroneously put in the initial draft of the essay, and it, too, could prove somewhat optimistic. 

My first guess was immediately wrong, here's a new guess that might also be immediately wrong.

> Perhaps my analysis is all wrong, even deeply flawed. But the stakes are too high to continue on the current course without reexamining the data and the erroneous models that are predicting doom.

The stakes are too high not to say completely flawed things!

Don't do this. It's not honestly conveying uncertainty, it's attempting to cover your ass from being held accountable later.

# Conclusion

The challenge in reviewing pieces like these lies in their incurious and insincere construction. This review took a substantial amount of time, and in the meantime the original piece was poorly revised, several interviews and a podcast were released, and a second post trying to cover for the first went live.^[And the schema for the underlying COVID-19 data changed breaking a lot of code.] More will no doubt soon continue to move the goal posts and argument.  It is an [order of magnitude less effort](https://en.wikipedia.org/wiki/Gish_gallop) to spam poorly constructed hypotheticals than it is to deconstruct them. In a world where actual life or death policy analysis is being treated like a high school debate round, the only strategic move is to step back, slow down, and draw methodological lessons for our students and colleagues that will apply to a broad set current and future analysis.

Epstein (2020a,2020a2,2020b) and analysis like it shouldn't be rejected because the author is out of their lane. It should be rejected because it's bad work. I'm not a epidemiologist either, but even as just as a social scientist I can show lots of different specific ways that the analysis fails to meet basic standards of scientific inference. An epidemiologist would have even more detailed empirically relevant issues to point out. In this current time of crisis, we should resist the urge to gate-keep and instead encourage honesty, curiosity, high standards, and good work. Even blatantly incurious and bad work can serve as a pedagogical tool to train young researchers what not to do. We need to take these opportunities to learn so that we are all smarter and better prepared for the next crisis.




